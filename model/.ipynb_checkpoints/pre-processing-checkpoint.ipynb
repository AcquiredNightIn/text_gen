{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text data\n",
    "\n",
    "The data is currently one text file, with each line corresponding to one post. The method for extraction from reddit is detailed in the scraper file. \n",
    "\n",
    "The data will first be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file imported\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/relationships_10000.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_relationship_data = file.read()\n",
    "    print(\"file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Had A Dream About A Past Flame And Woke Up Missing Them - Am I Crazy?\\nSister [11f] sleeps beside m'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_relationship_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first post this raises an interesting point about capital letters. I assumed that we wouldn't need to lowercase all the data, but if this Capitalised Every Word syntax is prevelant then this could be an issue. We will assume that lowercasing will produce a more informative model due to uniformity and lower likelyhood of Out Of Vocab words.\n",
    "\n",
    "We are going to explore the punctuation in the text as a whole to see what may be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of punctuations\n",
      "\\n   10000\n",
      ".   4417\n",
      ",   1809\n",
      ":   165\n",
      ";   33\n",
      "\\t   0\n",
      "?   3672\n",
      "!   328\n",
      "-   503\n",
      "(   7018\n",
      ")   6990\n",
      ":(   27\n",
      ":)   1\n",
      "</3   1\n",
      "[   2964\n",
      "]   2965\n",
      "'   2309\n",
      "\" 291\n",
      "<   3\n",
      "_   31\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of punctuations\")\n",
    "print(r\"\\n  \", raw_relationship_data.count(\"\\n\"))\n",
    "print(r\".  \", raw_relationship_data.count(\".\"))\n",
    "print(r\",  \", raw_relationship_data.count(\",\"))\n",
    "print(r\":  \", raw_relationship_data.count(\":\"))\n",
    "print(r\";  \", raw_relationship_data.count(\";\"))\n",
    "print(r\"\\t  \", raw_relationship_data.count(\"\\t\"))\n",
    "print(r\"?  \", raw_relationship_data.count(\"?\"))\n",
    "print(r\"!  \", raw_relationship_data.count(\"!\"))\n",
    "print(r\"-  \", raw_relationship_data.count(\"-\"))\n",
    "print(r\"(  \", raw_relationship_data.count(\"(\"))\n",
    "print(r\")  \", raw_relationship_data.count(\")\"))\n",
    "print(r\":(  \", raw_relationship_data.count(\":(\"))\n",
    "print(r\":)  \", raw_relationship_data.count(\":)\"))\n",
    "print(r\"</3  \", raw_relationship_data.count(\"</3\"))\n",
    "print(r\"[  \", raw_relationship_data.count(\"[\"))\n",
    "print(r\"]  \", raw_relationship_data.count(\"]\"))\n",
    "print(r\"'  \", raw_relationship_data.count(\"'\"))\n",
    "print(r'\"', raw_relationship_data.count('\"'))\n",
    "print(r\"<  \", raw_relationship_data.count(\"<\"))\n",
    "print(r\"_  \", raw_relationship_data.count(\"_\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have exactly the right number of `\\n` symbols. The other punctuation may not be relevant as there are not huge numbers of non full stops, question marks and (maybe?) commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We want the data to take into account certain grammatical and punctuation syntax. Therefore we are going to map certain symbols to another, and to indicate where the end of a sentence is. It must be ensured that there are adequate spaces between relevant tokens or they won't be parse properly. \n",
    "\n",
    "The punctuation that is going to be kept in is:\n",
    "\n",
    "* full stops\n",
    "* question marks\n",
    "* brackets (one type)\n",
    "\n",
    "We are going to convert the text to lower case for all words in order to increase the uniformity of the text.\n",
    "\n",
    "The newline `/n` symbol is going to be converted to ` <END> ` to indicate the end of a post (using the assumtion that posts are one line per post).\n",
    "\n",
    "Should probably be using regular expressions here for better performance but alas this is a first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had a dream about a past flame and woke up missing them - am i crazy?\n",
      "sister [11f] sleeps beside m\n"
     ]
    }
   ],
   "source": [
    "raw_relationship_data = raw_relationship_data.lower()\n",
    "print(raw_relationship_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spaces to the punctuation we want to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.replace(\"<\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\">\", \" \")\n",
    "\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"\\n\", \" <END> <START> \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\".\", \" . \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"?\", \" ? \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\",\", \" , \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"[\", \" (\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"]\", \") \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\":\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\";\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"-\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"!\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"_\", \" \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace('\"', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"'\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"“\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('”', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('’', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('…', \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace('...', \" , \")\n",
    "raw_relationship_data = raw_relationship_data.replace('/', \" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave up on not using regular expressions, we can check what non-alpha nums are still within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '.',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¿',\n",
       " 'á',\n",
       " 'ã',\n",
       " 'ç',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ô',\n",
       " 'ü',\n",
       " 'ă',\n",
       " 'ı',\n",
       " 'ť',\n",
       " 'а',\n",
       " 'в',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'и',\n",
       " 'к',\n",
       " 'л',\n",
       " 'м',\n",
       " 'о',\n",
       " 'р',\n",
       " 'с',\n",
       " 'т',\n",
       " 'х',\n",
       " 'ч',\n",
       " 'ы',\n",
       " 'ь',\n",
       " 'ấ',\n",
       " 'ẻ',\n",
       " 'ế',\n",
       " 'ố',\n",
       " 'ử',\n",
       " '\\u200d',\n",
       " '–',\n",
       " '—',\n",
       " '‘',\n",
       " '„',\n",
       " '€',\n",
       " '☺',\n",
       " '♀',\n",
       " '♂',\n",
       " '♡',\n",
       " '♥',\n",
       " '️',\n",
       " '𝐆',\n",
       " '𝐋',\n",
       " '𝐑',\n",
       " '𝐒',\n",
       " '𝐓',\n",
       " '𝐚',\n",
       " '𝐞',\n",
       " '𝐟',\n",
       " '𝐠',\n",
       " '𝐡',\n",
       " '𝐢',\n",
       " '𝐤',\n",
       " '𝐥',\n",
       " '𝐦',\n",
       " '𝐧',\n",
       " '𝐨',\n",
       " '𝐩',\n",
       " '𝐬',\n",
       " '𝐭',\n",
       " '𝟗',\n",
       " '🎹',\n",
       " '🏻',\n",
       " '🏼',\n",
       " '🏽',\n",
       " '👏',\n",
       " '👧',\n",
       " '💔',\n",
       " '💕',\n",
       " '💝',\n",
       " '🔥',\n",
       " '😅',\n",
       " '😔',\n",
       " '😞',\n",
       " '😩',\n",
       " '😪',\n",
       " '😬',\n",
       " '😭',\n",
       " '😲',\n",
       " '🤔',\n",
       " '🤕',\n",
       " '🤦',\n",
       " '🤷',\n",
       " '🥵',\n",
       " '🥺'}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "set(re.sub(r'[A-Za-z0-9 ]', '', raw_relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see there is a wide range of punctuation that is not covered by our replacing procedure. We will remove all:\n",
    "\n",
    "* alphanumerics\n",
    "* full stops, commas, question marks\n",
    "* characters in the `<END>` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had a dream about a past flame and woke up missing them  am i crazy ? <END> <START> sister (11f) sleeps beside my (26m) used t shirts because it helps her sleep while im not at home . i find it uncom\n"
     ]
    }
   ],
   "source": [
    "relationship_data = re.sub(r'^[A-Za-z0-9 <>,.?]', ' ', raw_relationship_data)\n",
    "relationship_data = relationship_data.replace(\"  \", \" \")\n",
    "print(relationship_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"  had a dream about a past flame and woke up missing them   am i crazy ?  <END>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That first post has gone wrong, for some reason due to the replacing or regular expressions. This problem with the initial \"I\" doesn't seem to be the case for the rest of the sentences. We will jsut strip the front. We end up keeping some parenthesis in as we want the (m23) type syntax, hopefully this will not impact the performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> sister (11f) sleeps be\n"
     ]
    }
   ],
   "source": [
    "relationship_data = relationship_data[77:]\n",
    "print(relationship_data[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (GENGER_AGE) syntax may be useful to replace with a generic placeholder in order to prevent rare / out of vocab issues, the model will end up predicting some age based on langauge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite sure where to tokenise this data, definitely before creating the sequences but not sure if the data should be sentences first.\n",
    "\n",
    "Will go with before creating sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separate the string into words using spaces to determine a new token. This will make punctuation tokens which is what we want for sentence structure.\n",
    "\n",
    "Could use one of NLTK's casual tokenizer but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  153\n"
     ]
    }
   ],
   "source": [
    "# this has to be done after tokenisation or it will count strings\n",
    "vocab = sorted(set(relationship_data))\n",
    "len_vocab = len(vocab)\n",
    "print(\"Vocab length: \", len_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
