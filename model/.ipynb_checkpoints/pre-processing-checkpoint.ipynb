{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text data\n",
    "\n",
    "The data is currently one text file, with each line corresponding to one post. The method for extraction from reddit is detailed in the scraper file. \n",
    "\n",
    "The data will first be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file imported\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/relationships_10000.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_relationship_data = file.read()\n",
    "    print(\"file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Had A Dream About A Past Flame And Woke Up Missing Them - Am I Crazy?\\nSister [11f] sleeps beside m'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_relationship_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first post this raises an interesting point about capital letters. I assumed that we wouldn't need to lowercase all the data, but if this Capitalised Every Word syntax is prevelant then this could be an issue. We will assume that lowercasing will produce a more informative model due to uniformity and lower likelyhood of Out Of Vocab words.\n",
    "\n",
    "We are going to explore the punctuation in the text as a whole to see what may be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of punctuations\n",
      "\\n   10000\n",
      ".   4417\n",
      ",   1809\n",
      ":   165\n",
      ";   33\n",
      "\\t   0\n",
      "?   3672\n",
      "!   328\n",
      "-   503\n",
      "(   7018\n",
      ")   6990\n",
      ":(   27\n",
      ":)   1\n",
      "</3   1\n",
      "[   2964\n",
      "]   2965\n",
      "'   2309\n",
      "\" 291\n",
      "<   3\n",
      "_   31\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of punctuations\")\n",
    "print(r\"\\n  \", raw_relationship_data.count(\"\\n\"))\n",
    "print(r\".  \", raw_relationship_data.count(\".\"))\n",
    "print(r\",  \", raw_relationship_data.count(\",\"))\n",
    "print(r\":  \", raw_relationship_data.count(\":\"))\n",
    "print(r\";  \", raw_relationship_data.count(\";\"))\n",
    "print(r\"\\t  \", raw_relationship_data.count(\"\\t\"))\n",
    "print(r\"?  \", raw_relationship_data.count(\"?\"))\n",
    "print(r\"!  \", raw_relationship_data.count(\"!\"))\n",
    "print(r\"-  \", raw_relationship_data.count(\"-\"))\n",
    "print(r\"(  \", raw_relationship_data.count(\"(\"))\n",
    "print(r\")  \", raw_relationship_data.count(\")\"))\n",
    "print(r\":(  \", raw_relationship_data.count(\":(\"))\n",
    "print(r\":)  \", raw_relationship_data.count(\":)\"))\n",
    "print(r\"</3  \", raw_relationship_data.count(\"</3\"))\n",
    "print(r\"[  \", raw_relationship_data.count(\"[\"))\n",
    "print(r\"]  \", raw_relationship_data.count(\"]\"))\n",
    "print(r\"'  \", raw_relationship_data.count(\"'\"))\n",
    "print(r'\"', raw_relationship_data.count('\"'))\n",
    "print(r\"<  \", raw_relationship_data.count(\"<\"))\n",
    "print(r\"_  \", raw_relationship_data.count(\"_\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have exactly the right number of `\\n` symbols. The other punctuation may not be relevant as there are not huge numbers of non full stops, question marks and (maybe?) commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We want the data to take into account certain grammatical and punctuation syntax. Therefore we are going to map certain symbols to another, and to indicate where the end of a sentence is. It must be ensured that there are adequate spaces between relevant tokens or they won't be parse properly. \n",
    "\n",
    "The punctuation that is going to be kept in is:\n",
    "\n",
    "* full stops\n",
    "* question marks\n",
    "* brackets (one type)\n",
    "\n",
    "We are going to convert the text to lower case for all words in order to increase the uniformity of the text.\n",
    "\n",
    "The newline `/n` symbol is going to be converted to ` <END> ` to indicate the end of a post (using the assumtion that posts are one line per post).\n",
    "\n",
    "Should probably be using regular expressions here for better performance but alas this is a first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had a dream about a past flame and woke up missing them - am i crazy?\n",
      "sister [11f] sleeps beside m\n"
     ]
    }
   ],
   "source": [
    "raw_relationship_data = raw_relationship_data.lower()\n",
    "print(raw_relationship_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spaces to the punctuation we want to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.replace(\"<\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\">\", \" \")\n",
    "\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"\\n\", \" <END> <START> \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\".\", \" . \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"?\", \" ? \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\",\", \" , \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"[\", \" (\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"]\", \") \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\":\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\";\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"-\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"!\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"_\", \" \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace('\"', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"'\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"‚Äú\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äù', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äô', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Ä¶', \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace('...', \" , \")\n",
    "#raw_relationship_data = raw_relationship_data.replace('/', \" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave up on not using regular expressions, we can check what non-alpha nums are still within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '.',\n",
       " '/',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¬ø',\n",
       " '√°',\n",
       " '√£',\n",
       " '√ß',\n",
       " '√©',\n",
       " '√™',\n",
       " '√¥',\n",
       " '√º',\n",
       " 'ƒÉ',\n",
       " 'ƒ±',\n",
       " '≈•',\n",
       " '–∞',\n",
       " '–≤',\n",
       " '–µ',\n",
       " '–∂',\n",
       " '–∏',\n",
       " '–∫',\n",
       " '–ª',\n",
       " '–º',\n",
       " '–æ',\n",
       " '—Ä',\n",
       " '—Å',\n",
       " '—Ç',\n",
       " '—Ö',\n",
       " '—á',\n",
       " '—ã',\n",
       " '—å',\n",
       " '·∫•',\n",
       " '·∫ª',\n",
       " '·∫ø',\n",
       " '·ªë',\n",
       " '·ª≠',\n",
       " '\\u200d',\n",
       " '‚Äì',\n",
       " '‚Äî',\n",
       " '‚Äò',\n",
       " '‚Äû',\n",
       " '‚Ç¨',\n",
       " '‚ò∫',\n",
       " '‚ôÄ',\n",
       " '‚ôÇ',\n",
       " '‚ô°',\n",
       " '‚ô•',\n",
       " 'Ô∏è',\n",
       " 'ùêÜ',\n",
       " 'ùêã',\n",
       " 'ùêë',\n",
       " 'ùêí',\n",
       " 'ùêì',\n",
       " 'ùêö',\n",
       " 'ùêû',\n",
       " 'ùêü',\n",
       " 'ùê†',\n",
       " 'ùê°',\n",
       " 'ùê¢',\n",
       " 'ùê§',\n",
       " 'ùê•',\n",
       " 'ùê¶',\n",
       " 'ùêß',\n",
       " 'ùê®',\n",
       " 'ùê©',\n",
       " 'ùê¨',\n",
       " 'ùê≠',\n",
       " 'ùüó',\n",
       " 'üéπ',\n",
       " 'üèª',\n",
       " 'üèº',\n",
       " 'üèΩ',\n",
       " 'üëè',\n",
       " 'üëß',\n",
       " 'üíî',\n",
       " 'üíï',\n",
       " 'üíù',\n",
       " 'üî•',\n",
       " 'üòÖ',\n",
       " 'üòî',\n",
       " 'üòû',\n",
       " 'üò©',\n",
       " 'üò™',\n",
       " 'üò¨',\n",
       " 'üò≠',\n",
       " 'üò≤',\n",
       " 'ü§î',\n",
       " 'ü§ï',\n",
       " 'ü§¶',\n",
       " 'ü§∑',\n",
       " 'ü•µ',\n",
       " 'ü•∫'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "set(re.sub(r'[A-Za-z0-9 ]', '', raw_relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see there is a wide range of punctuation that is not covered by our replacing procedure. We will remove all:\n",
    "\n",
    "* alphanumerics\n",
    "* full stops, commas, question marks\n",
    "* characters in the `<END>` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had a dream about a past flame and woke up missing them  am i crazy ? <END> <START> sister (11f) sleeps beside my (26m) used t shirts because it helps her sleep while im not at home . i find it uncom\n"
     ]
    }
   ],
   "source": [
    "relationship_data = re.sub(r'^[A-Za-z0-9 <>,.?]', ' ', raw_relationship_data)\n",
    "relationship_data = relationship_data.replace(\"  \", \" \")\n",
    "print(relationship_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"  had a dream about a past flame and woke up missing them   am i crazy ?  <END>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That first post has gone wrong, for some reason due to the replacing or regular expressions. This problem with the initial \"I\" doesn't seem to be the case for the rest of the sentences. We will jsut strip the front. We end up keeping some parenthesis in as we want the (m23) type syntax, hopefully this will not impact the performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> sister (11f) sleeps be\n"
     ]
    }
   ],
   "source": [
    "relationship_data = relationship_data[77:]\n",
    "print(relationship_data[:30])\n",
    "#relationship_data = relationship_data + \" <END>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (GENGER_AGE) syntax may be useful to replace with a generic placeholder in order to prevent rare / out of vocab issues, the model will end up predicting some age based on langauge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite sure where to tokenise this data, definitely before creating the sequences but not sure if the data should be sentences first.\n",
    "\n",
    "Will go with before creating sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separate the string into words using spaces to determine a new token. This will make punctuation tokens which is what we want for sentence structure.\n",
    "\n",
    "Could use one of NLTK's casual tokenizer but as we have already preprocessed the strings for our own purpose the standard one may do fine. EDIT: as we have processed out words and punctuation to have whitespace where appropriate the WhitespaceTokenizer is best here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'sister', '(11f)', 'sleeps', 'beside', 'my', '(26m)', 'used', 't', 'shirts', 'because', 'it', 'helps', 'her', 'sleep', 'while', 'im', 'not', 'at', 'home', '.', 'i', 'find', 'it', 'uncomfortable', 'but', 'also', 'im', 'not', 'sure', 'what', 'to', 'think', '.', 'is', 'this', 'normal/ok', '?', '<END>', '<START>', 'equality', 'in', 'relationship', '<END>', '<START>', 'r/relationship', 'i', 'need', 'your', 'perspective']\n"
     ]
    }
   ],
   "source": [
    "ws_tk = WhitespaceTokenizer() \n",
    "\n",
    "relationships_word_tokened = ws_tk.tokenize(relationship_data)\n",
    "\n",
    "print(relationships_word_tokened[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all_word_dist = FreqDist(word for word in relationships_word_tokened)\n",
    "# print(all_word_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly many of our most common words are stop words, but these are important to our sentence structure so they will be kept in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may choose the use the sentence structure of our data instead of a bag of words model, this will mean tokenising the sentences as well as words. I've done this kind of backwards as the `\\n` strings denoted new posts previously but now we get a string for each post that has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' sister (11f) sleeps beside my (26m) used t shirts because it helps her sleep while im not at home . i find it uncomfortable but also im not sure what to think . is this normal/ok ?', 'equality in relationship', 'r/relationship i need your perspective and help', 'my (34f) (ex)boyfriend (40m) cheated on me last night am i making the right decision ?', 'i (24m) react too intensely when my husband (23m) has a problem  how do i calm down ?', 'r/relationships i need your perspective', 'should i (24f) remain friends with my ex boyfriend (32m) ?', 'am i (m23) getting overly attached too quickly ?', 'how do i (24m) stop reacting so intensely ?', 'i (30f) have a weird (abusive ? ) relationship with my boss (36f) and may need to quit abruptly . no idea what to do']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents = relationship_data.split(\" <END> <START> \")\n",
    "relationship_data_sents[0] = relationship_data_sents[0].replace(\"<START>\", \"\")\n",
    "relationship_data_sents = [x for x in relationship_data_sents if x]\n",
    "\n",
    "\n",
    "print(relationship_data_sents[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max post length:  73 \n",
      "\n",
      "\n",
      "Min post length:  1 \n",
      "\n",
      "\n",
      "['my', '(34f)', '(ex)boyfriend', '(40m)', 'cheated', 'on', 'me', 'last', 'night', 'am', 'i', 'making', 'the', 'right', 'decision', '?']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents_words = [ws_tk.tokenize(post) for post in relationship_data_sents]\n",
    "\n",
    "MAX_SEQ_LENGTH = max([len(post) for post in relationship_data_sents_words])\n",
    "\n",
    "relationship_data_sents_words = [x for x in relationship_data_sents_words if x]\n",
    "\n",
    "MIN_SEQ_LENGTH = min([len(post) for post in relationship_data_sents_words])\n",
    "\n",
    "\n",
    "print(\"Max post length: \", MAX_SEQ_LENGTH, \"\\n\\n\")\n",
    "print(\"Min post length: \", MIN_SEQ_LENGTH, \"\\n\\n\")\n",
    "\n",
    "\n",
    "print(relationship_data_sents_words[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing each post, within each post is a list of each token within the post. The longest post is given by `MAX_SEQ_LENGTH`\n",
    "\n",
    "### Generate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  7434\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "flattened_word_tokened = functools.reduce(operator.concat, relationship_data_sents_words)\n",
    "\n",
    "# this has to be done after tokenisation or it will count strings\n",
    "vocab = sorted(set(flattened_word_tokened))\n",
    "len_vocab = len(vocab) + 1\n",
    "print(\"Vocab length: \", len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the word data into integers the model will be able to understand, a little bit cheating but keras has a nice way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "keras_embedder = Tokenizer(num_words=None, filters=[], lower=False, split=\" \")\n",
    "\n",
    "keras_embedder.fit_on_texts(relationship_data_sents)\n",
    "\n",
    "embedded_sents = keras_embedder.texts_to_sequences(relationship_data_sents)\n",
    "\n",
    "print(len(embedded_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an embedding for each post. We can now make the train/\n",
    "predict seqence pairs. We now have an ordered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 374538\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "embedded_sent_ = []\n",
    "for post in embedded_sents:\n",
    "    if len(post) > 3:\n",
    "        embedded_sent_.append(post)\n",
    "\n",
    "sequences = []\n",
    "for post in embedded_sent_:\n",
    "    for index in range(2, len(post)):\n",
    "        single_sequence = post[index-2:index+1]\n",
    "        sequences.append(single_sequence)\n",
    "\n",
    "print(\"Total Sequences: {}\".format(len(functools.reduce(operator.concat, sequences))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to ensure all our sequences are padded adequately, they should be now by prunning non-3 lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124846, 3)\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, maxlen=3, padding='pre')\n",
    "        \n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences to features + targets in order to train a model in a categorical manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124846, 2)\n",
      "(124846,)\n",
      "[4283 4284    2  147  356 2378 4285   75   21 3014   24  447  194   23\n",
      "   38   64  303    3    1  300]\n"
     ]
    }
   ],
   "source": [
    "# split into input and output elements\n",
    "X = []\n",
    "y = []\n",
    "padded_sequences = [each for each in padded_sequences if each != []]\n",
    "padded_sequences = [each for each in padded_sequences if len(each) > 1]\n",
    "for each_post in padded_sequences:\n",
    "    each_post = np.array(each_post)\n",
    "    X_each_post, y_each_post = each_post[:-1], each_post[-1]\n",
    "    X.append(X_each_post)\n",
    "    y.append(y_each_post)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 50)             371700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7434)              1910538   \n",
      "=================================================================\n",
      "Total params: 2,596,606\n",
      "Trainable params: 2,596,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "number_of_embeddings = 50\n",
    "LSTM_units = 256\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len_vocab, number_of_embeddings, input_length=2))\n",
    "model.add(LSTM(LSTM_units))\n",
    "model.add(Dense(len_vocab, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/10\n",
      "124846/124846 [==============================] - 124s 994us/step - loss: 5.8684 - acc: 0.0960\n",
      "Epoch 2/10\n",
      "124846/124846 [==============================] - 125s 998us/step - loss: 4.9004 - acc: 0.1769\n",
      "Epoch 3/10\n",
      "124846/124846 [==============================] - 126s 1ms/step - loss: 4.4714 - acc: 0.2097\n",
      "Epoch 4/10\n",
      "124846/124846 [==============================] - 126s 1ms/step - loss: 4.1263 - acc: 0.2368\n",
      "Epoch 5/10\n",
      "124846/124846 [==============================] - 126s 1ms/step - loss: 3.8187 - acc: 0.2617\n",
      "Epoch 6/10\n",
      "124846/124846 [==============================] - 128s 1ms/step - loss: 3.5554 - acc: 0.2867\n",
      "Epoch 7/10\n",
      "124846/124846 [==============================] - 131s 1ms/step - loss: 3.3329 - acc: 0.3111\n",
      "Epoch 8/10\n",
      "124846/124846 [==============================] - 131s 1ms/step - loss: 3.1477 - acc: 0.3333\n",
      "Epoch 9/10\n",
      "124846/124846 [==============================] - 130s 1ms/step - loss: 2.9884 - acc: 0.3539\n",
      "Epoch 10/10\n",
      "124846/124846 [==============================] - 130s 1ms/step - loss: 2.8546 - acc: 0.3720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f3835c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "num_epochs = 10\n",
    "\n",
    "model.fit(X, y, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that will convert from the embedded numbers to text with a number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there is something wrong with me ? ? ? ? ? ?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 3\n",
    "\n",
    "test = \"there is\"\n",
    "\n",
    "generate_seq(model, keras_embedder, max_length-1, test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife (34f) and i dont know what to do . . . . . . . . . . . . . . . . . \n",
      "\n",
      "My husband (30m) is having a hard time moving forward in the wrong ? chasing empathy . . . . . . . . . . . \n",
      "\n",
      "My friend (m19) has a girlfriend . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "My fiance (30m) of 7 months , but i have a lot of suspicion about the future ? üíî and i dont know what to do . \n",
      "\n",
      "My (22M) over possibly being evaluated for autism and i dont know what to do . . . . . . . . . . . . \n",
      "\n",
      "My girlfriend (23/f) of 1 year , and i dont know what to do . . . . . . . . . . . . . \n",
      "\n",
      "My boyfriend (m19) doesnt like me ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
      "\n",
      "My partner of 1 year , and i dont know what to do . . . . . . . . . . . . . . \n",
      "\n",
      "My (23F) over possibly being evaluated for autism and i dont know what to do . . . . . . . . . . . . \n",
      "\n",
      "My spouse (30m) is having a hard time moving forward in the wrong ? chasing empathy . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1 = \"My wife\"\n",
    "test2 = \"My husband\"\n",
    "test3 = \"My friend\"\n",
    "test4 = \"My fiance\"\n",
    "test5 = \"My (22M)\"\n",
    "test6 = \"My girlfriend\"\n",
    "test7 = \"My boyfriend\"\n",
    "test8 = \"My partner\"\n",
    "test9 = \"My (23F)\"\n",
    "test10 = \"My spouse\"\n",
    "\n",
    "length = 25\n",
    "\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test1, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test2, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test3, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test4, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test5, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test6, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test7, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test8, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test9, length),\"\\n\")\n",
    "print(generate_seq(model, keras_embedder, max_length-1, test10, length),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
