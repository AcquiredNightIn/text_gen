{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import FreqDist\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text data\n",
    "\n",
    "The data is currently one text file, with each line corresponding to one post. The method for extraction from reddit is detailed in the scraper file. \n",
    "\n",
    "The data will first be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file imported\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/relationships_10000.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_relationship_data = file.read()\n",
    "    print(\"file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Had A Dream About A Past Flame And Woke Up Missing Them - Am I Crazy?\\nSister [11f] sleeps beside m'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_relationship_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first post this raises an interesting point about capital letters. I assumed that we wouldn't need to lowercase all the data, but if this Capitalised Every Word syntax is prevelant then this could be an issue. We will assume that lowercasing will produce a more informative model due to uniformity and lower likelyhood of Out Of Vocab words.\n",
    "\n",
    "We are going to explore the punctuation in the text as a whole to see what may be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of punctuations\n",
      "\\n   10000\n",
      ".   4417\n",
      ",   1809\n",
      ":   165\n",
      ";   33\n",
      "\\t   0\n",
      "?   3672\n",
      "!   328\n",
      "-   503\n",
      "(   7018\n",
      ")   6990\n",
      ":(   27\n",
      ":)   1\n",
      "</3   1\n",
      "[   2964\n",
      "]   2965\n",
      "'   2309\n",
      "\" 291\n",
      "<   3\n",
      "_   31\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of punctuations\")\n",
    "print(r\"\\n  \", raw_relationship_data.count(\"\\n\"))\n",
    "print(r\".  \", raw_relationship_data.count(\".\"))\n",
    "print(r\",  \", raw_relationship_data.count(\",\"))\n",
    "print(r\":  \", raw_relationship_data.count(\":\"))\n",
    "print(r\";  \", raw_relationship_data.count(\";\"))\n",
    "print(r\"\\t  \", raw_relationship_data.count(\"\\t\"))\n",
    "print(r\"?  \", raw_relationship_data.count(\"?\"))\n",
    "print(r\"!  \", raw_relationship_data.count(\"!\"))\n",
    "print(r\"-  \", raw_relationship_data.count(\"-\"))\n",
    "print(r\"(  \", raw_relationship_data.count(\"(\"))\n",
    "print(r\")  \", raw_relationship_data.count(\")\"))\n",
    "print(r\":(  \", raw_relationship_data.count(\":(\"))\n",
    "print(r\":)  \", raw_relationship_data.count(\":)\"))\n",
    "print(r\"</3  \", raw_relationship_data.count(\"</3\"))\n",
    "print(r\"[  \", raw_relationship_data.count(\"[\"))\n",
    "print(r\"]  \", raw_relationship_data.count(\"]\"))\n",
    "print(r\"'  \", raw_relationship_data.count(\"'\"))\n",
    "print(r'\"', raw_relationship_data.count('\"'))\n",
    "print(r\"<  \", raw_relationship_data.count(\"<\"))\n",
    "print(r\"_  \", raw_relationship_data.count(\"_\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have exactly the right number of `\\n` symbols. The other punctuation may not be relevant as there are not huge numbers of non full stops, question marks and (maybe?) commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We want the data to take into account certain grammatical and punctuation syntax. Therefore we are going to map certain symbols to another, and to indicate where the end of a sentence is. It must be ensured that there are adequate spaces between relevant tokens or they won't be parse properly. \n",
    "\n",
    "The punctuation that is going to be kept in is:\n",
    "\n",
    "* full stops\n",
    "* question marks\n",
    "* brackets (one type)\n",
    "\n",
    "We are going to convert the text to lower case for all words in order to increase the uniformity of the text.\n",
    "\n",
    "The newline `/n` symbol is going to be converted to ` <END> ` to indicate the end of a post (using the assumtion that posts are one line per post).\n",
    "\n",
    "Should probably be using regular expressions here for better performance but alas this is a first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had a dream about a past flame and woke up missing them - am i crazy?\n",
      "sister [11f] sleeps beside m\n"
     ]
    }
   ],
   "source": [
    "raw_relationship_data = raw_relationship_data.lower()\n",
    "print(raw_relationship_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spaces to the punctuation we want to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.replace(\"<\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\">\", \" \")\n",
    "\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"\\n\", \" <END> <START> \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\".\", \" . \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"?\", \" ? \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\",\", \" , \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"[\", \" (\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"]\", \") \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\":\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\";\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"-\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"!\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"_\", \" \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace('\"', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"'\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"‚Äú\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äù', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äô', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Ä¶', \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace('...', \" , \")\n",
    "#raw_relationship_data = raw_relationship_data.replace('/', \" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave up on not using regular expressions, we can check what non-alpha nums are still within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '.',\n",
       " '/',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¬ø',\n",
       " '√°',\n",
       " '√£',\n",
       " '√ß',\n",
       " '√©',\n",
       " '√™',\n",
       " '√¥',\n",
       " '√º',\n",
       " 'ƒÉ',\n",
       " 'ƒ±',\n",
       " '≈•',\n",
       " '–∞',\n",
       " '–≤',\n",
       " '–µ',\n",
       " '–∂',\n",
       " '–∏',\n",
       " '–∫',\n",
       " '–ª',\n",
       " '–º',\n",
       " '–æ',\n",
       " '—Ä',\n",
       " '—Å',\n",
       " '—Ç',\n",
       " '—Ö',\n",
       " '—á',\n",
       " '—ã',\n",
       " '—å',\n",
       " '·∫•',\n",
       " '·∫ª',\n",
       " '·∫ø',\n",
       " '·ªë',\n",
       " '·ª≠',\n",
       " '\\u200d',\n",
       " '‚Äì',\n",
       " '‚Äî',\n",
       " '‚Äò',\n",
       " '‚Äû',\n",
       " '‚Ç¨',\n",
       " '‚ò∫',\n",
       " '‚ôÄ',\n",
       " '‚ôÇ',\n",
       " '‚ô°',\n",
       " '‚ô•',\n",
       " 'Ô∏è',\n",
       " 'ùêÜ',\n",
       " 'ùêã',\n",
       " 'ùêë',\n",
       " 'ùêí',\n",
       " 'ùêì',\n",
       " 'ùêö',\n",
       " 'ùêû',\n",
       " 'ùêü',\n",
       " 'ùê†',\n",
       " 'ùê°',\n",
       " 'ùê¢',\n",
       " 'ùê§',\n",
       " 'ùê•',\n",
       " 'ùê¶',\n",
       " 'ùêß',\n",
       " 'ùê®',\n",
       " 'ùê©',\n",
       " 'ùê¨',\n",
       " 'ùê≠',\n",
       " 'ùüó',\n",
       " 'üéπ',\n",
       " 'üèª',\n",
       " 'üèº',\n",
       " 'üèΩ',\n",
       " 'üëè',\n",
       " 'üëß',\n",
       " 'üíî',\n",
       " 'üíï',\n",
       " 'üíù',\n",
       " 'üî•',\n",
       " 'üòÖ',\n",
       " 'üòî',\n",
       " 'üòû',\n",
       " 'üò©',\n",
       " 'üò™',\n",
       " 'üò¨',\n",
       " 'üò≠',\n",
       " 'üò≤',\n",
       " 'ü§î',\n",
       " 'ü§ï',\n",
       " 'ü§¶',\n",
       " 'ü§∑',\n",
       " 'ü•µ',\n",
       " 'ü•∫'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "set(re.sub(r'[A-Za-z0-9 ]', '', raw_relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see there is a wide range of punctuation that is not covered by our replacing procedure. We will remove all:\n",
    "\n",
    "* alphanumerics\n",
    "* full stops, commas, question marks\n",
    "* characters in the `<END>` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had a dream about a past flame and woke up missing them  am i crazy ? <END> <START> sister (11f) sleeps beside my (26m) used t shirts because it helps her sleep while im not at home . i find it uncom\n"
     ]
    }
   ],
   "source": [
    "relationship_data = re.sub(r'^[A-Za-z0-9 <>,.?]', ' ', raw_relationship_data)\n",
    "relationship_data = relationship_data.replace(\"  \", \" \")\n",
    "print(relationship_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"  had a dream about a past flame and woke up missing them   am i crazy ?  <END>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That first post has gone wrong, for some reason due to the replacing or regular expressions. This problem with the initial \"I\" doesn't seem to be the case for the rest of the sentences. We will jsut strip the front. We end up keeping some parenthesis in as we want the (m23) type syntax, hopefully this will not impact the performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> sister (11f) sleeps be\n"
     ]
    }
   ],
   "source": [
    "relationship_data = relationship_data[77:]\n",
    "print(relationship_data[:30])\n",
    "#relationship_data = relationship_data + \" <END>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (GENGER_AGE) syntax may be useful to replace with a generic placeholder in order to prevent rare / out of vocab issues, the model will end up predicting some age based on langauge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite sure where to tokenise this data, definitely before creating the sequences but not sure if the data should be sentences first.\n",
    "\n",
    "Will go with before creating sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separate the string into words using spaces to determine a new token. This will make punctuation tokens which is what we want for sentence structure.\n",
    "\n",
    "Could use one of NLTK's casual tokenizer but as we have already preprocessed the strings for our own purpose the standard one may do fine. EDIT: as we have processed out words and punctuation to have whitespace where appropriate the WhitespaceTokenizer is best here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'sister', '(11f)', 'sleeps', 'beside', 'my', '(26m)', 'used', 't', 'shirts', 'because', 'it', 'helps', 'her', 'sleep', 'while', 'im', 'not', 'at', 'home', '.', 'i', 'find', 'it', 'uncomfortable', 'but', 'also', 'im', 'not', 'sure', 'what', 'to', 'think', '.', 'is', 'this', 'normal/ok', '?', '<END>', '<START>', 'equality', 'in', 'relationship', '<END>', '<START>', 'r/relationship', 'i', 'need', 'your', 'perspective']\n"
     ]
    }
   ],
   "source": [
    "ws_tk = WhitespaceTokenizer() \n",
    "\n",
    "relationships_word_tokened = ws_tk.tokenize(relationship_data)\n",
    "\n",
    "print(relationships_word_tokened[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly many of our most common words are stop words, but these are important to our sentence structure so they will be kept in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may choose the use the sentence structure of our data instead of a bag of words model, this will mean tokenising the sentences as well as words. I've done this kind of backwards as the `\\n` strings denoted new posts previously but now we get a string for each post that has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' sister (11f) sleeps beside my (26m) used t shirts because it helps her sleep while im not at home . i find it uncomfortable but also im not sure what to think . is this normal/ok ?', 'equality in relationship', 'r/relationship i need your perspective and help', 'my (34f) (ex)boyfriend (40m) cheated on me last night am i making the right decision ?', 'i (24m) react too intensely when my husband (23m) has a problem  how do i calm down ?', 'r/relationships i need your perspective', 'should i (24f) remain friends with my ex boyfriend (32m) ?', 'am i (m23) getting overly attached too quickly ?', 'how do i (24m) stop reacting so intensely ?', 'i (30f) have a weird (abusive ? ) relationship with my boss (36f) and may need to quit abruptly . no idea what to do']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents = relationship_data.split(\" <END> <START> \")\n",
    "relationship_data_sents[0] = relationship_data_sents[0].replace(\"<START>\", \"\")\n",
    "#relationship_data_sents = [x for x in relationship_data_sents if x]\n",
    "\n",
    "\n",
    "print(relationship_data_sents[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max post length:  73 \n",
      "\n",
      "\n",
      "Min post length:  1 \n",
      "\n",
      "\n",
      "['my', '(34f)', '(ex)boyfriend', '(40m)', 'cheated', 'on', 'me', 'last', 'night', 'am', 'i', 'making', 'the', 'right', 'decision', '?']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents_words = [ws_tk.tokenize(post) for post in relationship_data_sents]\n",
    "\n",
    "MAX_SEQ_LENGTH = max([len(post) for post in relationship_data_sents_words])\n",
    "\n",
    "relationship_data_sents_words = [x for x in relationship_data_sents_words if x]\n",
    "\n",
    "MIN_SEQ_LENGTH = min([len(post) for post in relationship_data_sents_words])\n",
    "\n",
    "\n",
    "print(\"Max post length: \", MAX_SEQ_LENGTH, \"\\n\\n\")\n",
    "print(\"Min post length: \", MIN_SEQ_LENGTH, \"\\n\\n\")\n",
    "\n",
    "\n",
    "print(relationship_data_sents_words[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing each post, within each post is a list of each token within the post. The longest post is given by `MAX_SEQ_LENGTH`\n",
    "\n",
    "### Generate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  7434\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "flattened_word_tokened = functools.reduce(operator.concat, relationship_data_sents_words)\n",
    "\n",
    "# this has to be done after tokenisation or it will count strings\n",
    "vocab = sorted(set(flattened_word_tokened))\n",
    "len_vocab = len(vocab) + 1\n",
    "print(\"Vocab length: \", len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the word data into integers the model will be able to understand, a little bit cheating but keras has a nice way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "keras_embedder = Tokenizer(num_words=None, filters=[], lower=False, split=\" \")\n",
    "\n",
    "keras_embedder.fit_on_texts(relationship_data_sents)\n",
    "\n",
    "embedded_sents = keras_embedder.texts_to_sequences(relationship_data_sents)\n",
    "\n",
    "print(len(embedded_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an embedding for each post. We can now make the train/\n",
    "predict seqence pairs. We now have an ordered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 374538\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#embedded_sent_ = []\n",
    "#for post in embedded_sents:\n",
    "#    if len(post) > 3:\n",
    "#        embedded_sent_.append(post)\n",
    "\n",
    "sequences = []\n",
    "for post in embedded_sents:\n",
    "    if len(post) > 3:\n",
    "        for index in range(2, len(post)):\n",
    "            single_sequence = post[index-2:index+1]\n",
    "            sequences.append(single_sequence)\n",
    "\n",
    "print(\"Total Sequences: {}\".format(len(functools.reduce(operator.concat, sequences))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to ensure all our sequences are padded adequately, they should be now by prunning non-3 lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124846, 3)\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, maxlen=3, padding='pre')\n",
    "        \n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences to features + targets in order to train a model in a categorical manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124846, 2)\n",
      "(124846,)\n",
      "[   8  253    2 7046   45  927   22  115  672  493  253   43  342   55\n",
      "  577  154   70  264   91   23]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# split into input and output elements\n",
    "X = []\n",
    "y = []\n",
    "padded_sequences = [each for each in padded_sequences if each != []]\n",
    "padded_sequences = [each for each in padded_sequences if len(each) > 1]\n",
    "for each_post in padded_sequences:\n",
    "    each_post = np.array(each_post)\n",
    "    X_each_post, y_each_post = each_post[:-1], each_post[-1]\n",
    "    X.append(X_each_post)\n",
    "    y.append(y_each_post)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "pickle_file = \"../processed_data/{}\".format(timestr)\n",
    "\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump((X, y, keras_embedder, len_vocab), f)\n",
    "\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    a,b,c,d = pickle.load(f) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
