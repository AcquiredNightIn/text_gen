{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text data\n",
    "\n",
    "The data is currently one text file, with each line corresponding to one post. The method for extraction from reddit is detailed in the scraper file. \n",
    "\n",
    "The data will first be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file imported\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/relationships_10000.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_relationship_data = file.read()\n",
    "    print(\"file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Had A Dream About A Past Flame And Woke Up Missing Them - Am I Crazy?\\nSister [11f] sleeps beside m'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_relationship_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first post this raises an interesting point about capital letters. I assumed that we wouldn't need to lowercase all the data, but if this Capitalised Every Word syntax is prevelant then this could be an issue. We will assume that lowercasing will produce a more informative model due to uniformity and lower likelyhood of Out Of Vocab words.\n",
    "\n",
    "We are going to explore the punctuation in the text as a whole to see what may be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of punctuations\n",
      "\\n   10000\n",
      ".   4417\n",
      ",   1809\n",
      ":   165\n",
      ";   33\n",
      "\\t   0\n",
      "?   3672\n",
      "!   328\n",
      "-   503\n",
      "(   7018\n",
      ")   6990\n",
      ":(   27\n",
      ":)   1\n",
      "</3   1\n",
      "[   2964\n",
      "]   2965\n",
      "'   2309\n",
      "\" 291\n",
      "<   3\n",
      "_   31\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of punctuations\")\n",
    "print(r\"\\n  \", raw_relationship_data.count(\"\\n\"))\n",
    "print(r\".  \", raw_relationship_data.count(\".\"))\n",
    "print(r\",  \", raw_relationship_data.count(\",\"))\n",
    "print(r\":  \", raw_relationship_data.count(\":\"))\n",
    "print(r\";  \", raw_relationship_data.count(\";\"))\n",
    "print(r\"\\t  \", raw_relationship_data.count(\"\\t\"))\n",
    "print(r\"?  \", raw_relationship_data.count(\"?\"))\n",
    "print(r\"!  \", raw_relationship_data.count(\"!\"))\n",
    "print(r\"-  \", raw_relationship_data.count(\"-\"))\n",
    "print(r\"(  \", raw_relationship_data.count(\"(\"))\n",
    "print(r\")  \", raw_relationship_data.count(\")\"))\n",
    "print(r\":(  \", raw_relationship_data.count(\":(\"))\n",
    "print(r\":)  \", raw_relationship_data.count(\":)\"))\n",
    "print(r\"</3  \", raw_relationship_data.count(\"</3\"))\n",
    "print(r\"[  \", raw_relationship_data.count(\"[\"))\n",
    "print(r\"]  \", raw_relationship_data.count(\"]\"))\n",
    "print(r\"'  \", raw_relationship_data.count(\"'\"))\n",
    "print(r'\"', raw_relationship_data.count('\"'))\n",
    "print(r\"<  \", raw_relationship_data.count(\"<\"))\n",
    "print(r\"_  \", raw_relationship_data.count(\"_\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have exactly the right number of `\\n` symbols. The other punctuation may not be relevant as there are not huge numbers of non full stops, question marks and (maybe?) commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We want the data to take into account certain grammatical and punctuation syntax. Therefore we are going to map certain symbols to another, and to indicate where the end of a sentence is. It must be ensured that there are adequate spaces between relevant tokens or they won't be parse properly. \n",
    "\n",
    "The punctuation that is going to be kept in is:\n",
    "\n",
    "* full stops\n",
    "* question marks\n",
    "* brackets (one type)\n",
    "\n",
    "We are going to convert the text to lower case for all words in order to increase the uniformity of the text.\n",
    "\n",
    "The newline `/n` symbol is going to be converted to ` <END> ` to indicate the end of a post (using the assumtion that posts are one line per post).\n",
    "\n",
    "Should probably be using regular expressions here for better performance but alas this is a first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had a dream about a past flame and woke up missing them - am i crazy?\n",
      "sister [11f] sleeps beside m\n"
     ]
    }
   ],
   "source": [
    "raw_relationship_data = raw_relationship_data.lower()\n",
    "print(raw_relationship_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spaces to the punctuation we want to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.replace(\"<\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\">\", \" \")\n",
    "\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"\\n\", \" <END> <START> \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\".\", \" . \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"?\", \" ? \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\",\", \" , \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"[\", \" (\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"]\", \") \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\":\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\";\", \" \")\n",
    "#raw_relationship_data = raw_relationship_data.replace(\"-\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"!\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"_\", \" \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace('\"', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"'\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"‚Äú\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äù', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Äô', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('‚Ä¶', \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace('...', \" , \")\n",
    "#raw_relationship_data = raw_relationship_data.replace('/', \" \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave up on not using regular expressions, we can check what non-alpha nums are still within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¬ø',\n",
       " '√°',\n",
       " '√£',\n",
       " '√ß',\n",
       " '√©',\n",
       " '√™',\n",
       " '√¥',\n",
       " '√º',\n",
       " 'ƒÉ',\n",
       " 'ƒ±',\n",
       " '≈•',\n",
       " '–∞',\n",
       " '–≤',\n",
       " '–µ',\n",
       " '–∂',\n",
       " '–∏',\n",
       " '–∫',\n",
       " '–ª',\n",
       " '–º',\n",
       " '–æ',\n",
       " '—Ä',\n",
       " '—Å',\n",
       " '—Ç',\n",
       " '—Ö',\n",
       " '—á',\n",
       " '—ã',\n",
       " '—å',\n",
       " '·∫•',\n",
       " '·∫ª',\n",
       " '·∫ø',\n",
       " '·ªë',\n",
       " '·ª≠',\n",
       " '\\u200d',\n",
       " '‚Äì',\n",
       " '‚Äî',\n",
       " '‚Äò',\n",
       " '‚Äû',\n",
       " '‚Ç¨',\n",
       " '‚ò∫',\n",
       " '‚ôÄ',\n",
       " '‚ôÇ',\n",
       " '‚ô°',\n",
       " '‚ô•',\n",
       " 'Ô∏è',\n",
       " 'ùêÜ',\n",
       " 'ùêã',\n",
       " 'ùêë',\n",
       " 'ùêí',\n",
       " 'ùêì',\n",
       " 'ùêö',\n",
       " 'ùêû',\n",
       " 'ùêü',\n",
       " 'ùê†',\n",
       " 'ùê°',\n",
       " 'ùê¢',\n",
       " 'ùê§',\n",
       " 'ùê•',\n",
       " 'ùê¶',\n",
       " 'ùêß',\n",
       " 'ùê®',\n",
       " 'ùê©',\n",
       " 'ùê¨',\n",
       " 'ùê≠',\n",
       " 'ùüó',\n",
       " 'üéπ',\n",
       " 'üèª',\n",
       " 'üèº',\n",
       " 'üèΩ',\n",
       " 'üëè',\n",
       " 'üëß',\n",
       " 'üíî',\n",
       " 'üíï',\n",
       " 'üíù',\n",
       " 'üî•',\n",
       " 'üòÖ',\n",
       " 'üòî',\n",
       " 'üòû',\n",
       " 'üò©',\n",
       " 'üò™',\n",
       " 'üò¨',\n",
       " 'üò≠',\n",
       " 'üò≤',\n",
       " 'ü§î',\n",
       " 'ü§ï',\n",
       " 'ü§¶',\n",
       " 'ü§∑',\n",
       " 'ü•µ',\n",
       " 'ü•∫'}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "set(re.sub(r'[A-Za-z0-9 ]', '', raw_relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see there is a wide range of punctuation that is not covered by our replacing procedure. We will remove all:\n",
    "\n",
    "* alphanumerics\n",
    "* full stops, commas, question marks\n",
    "* characters in the `<END>` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " had a dream about a past flame and woke up missing them - am i crazy ? <END> <START> sister (11f) sleeps beside my (26m) used t-shirts because it helps her sleep while im not at home . i find it unco\n"
     ]
    }
   ],
   "source": [
    "relationship_data = re.sub(r'^[A-Za-z0-9 <>,.?]', ' ', raw_relationship_data)\n",
    "relationship_data = relationship_data.replace(\"  \", \" \")\n",
    "print(relationship_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"  had a dream about a past flame and woke up missing them   am i crazy ?  <END>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That first post has gone wrong, for some reason due to the replacing or regular expressions. This problem with the initial \"I\" doesn't seem to be the case for the rest of the sentences. We will jsut strip the front. We end up keeping some parenthesis in as we want the (m23) type syntax, hopefully this will not impact the performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <START> sister (11f) sleeps b\n"
     ]
    }
   ],
   "source": [
    "relationship_data = relationship_data[77:]\n",
    "print(relationship_data[:30])\n",
    "#relationship_data = relationship_data + \" <END>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (GENGER_AGE) syntax may be useful to replace with a generic placeholder in order to prevent rare / out of vocab issues, the model will end up predicting some age based on langauge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite sure where to tokenise this data, definitely before creating the sequences but not sure if the data should be sentences first.\n",
    "\n",
    "Will go with before creating sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separate the string into words using spaces to determine a new token. This will make punctuation tokens which is what we want for sentence structure.\n",
    "\n",
    "Could use one of NLTK's casual tokenizer but as we have already preprocessed the strings for our own purpose the standard one may do fine. EDIT: as we have processed out words and punctuation to have whitespace where appropriate the WhitespaceTokenizer is best here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'sister', '(11f)', 'sleeps', 'beside', 'my', '(26m)', 'used', 't-shirts', 'because', 'it', 'helps', 'her', 'sleep', 'while', 'im', 'not', 'at', 'home', '.', 'i', 'find', 'it', 'uncomfortable', 'but', 'also', 'im', 'not', 'sure', 'what', 'to', 'think', '.', 'is', 'this', 'normal/ok', '?', '<END>', '<START>', 'equality', 'in', 'relationship', '<END>', '<START>', 'r/relationship', 'i', 'need', 'your', 'perspective', 'and']\n"
     ]
    }
   ],
   "source": [
    "ws_tk = WhitespaceTokenizer() \n",
    "\n",
    "relationships_word_tokened = ws_tk.tokenize(relationship_data)\n",
    "\n",
    "print(relationships_word_tokened[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  7590\n"
     ]
    }
   ],
   "source": [
    "# this has to be done after tokenisation or it will count strings\n",
    "vocab = sorted(set(relationships_word_tokened))\n",
    "len_vocab = len(vocab)\n",
    "print(\"Vocab length: \", len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<START>', 10000), ('<END>', 9999), ('i', 6467), ('my', 5667), ('.', 4417), ('to', 4075), ('?', 3671), ('a', 2993), ('and', 2855), ('me', 2447), ('with', 2378), (',', 1809), ('is', 1623), ('of', 1555), ('the', 1458), ('do', 1380), ('how', 1359), ('for', 1302), ('in', 1225), ('boyfriend', 1118), ('am', 1039), ('on', 1017), ('it', 983), ('relationship', 962), ('im', 910), ('her', 883), ('friend', 813), ('what', 801), ('have', 760), ('about', 756), ('that', 726), ('girlfriend', 722), ('up', 714), ('dont', 713), ('but', 660), ('he', 653), ('know', 642), ('should', 629), ('not', 615), ('ex', 604), ('out', 598), ('she', 575), ('this', 571), ('or', 551), ('you', 534), ('him', 527), ('like', 521), ('want', 507), ('feel', 495), ('be', 493)]\n"
     ]
    }
   ],
   "source": [
    "all_word_dist = FreqDist(word for word in relationships_word_tokened)\n",
    "print(all_word_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly many of our most common words are stop words, but these are important to our sentence structure so they will be kept in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may choose the use the sentence structure of our data instead of a bag of words model, this will mean tokenising the sentences as well as words. I've done this kind of backwards as the `\\n` strings denoted new posts previously but now we get a string for each post that has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max post length =  306 \n",
      "\n",
      "\n",
      "['  sister (11f) sleeps beside my (26m) used t-shirts because it helps her sleep while im not at home . i find it uncomfortable but also im not sure what to think . is this normal/ok ?', 'equality in relationship', 'r/relationship i need your perspective and help', 'my (34f) (ex)boyfriend (40m) cheated on me last night- am i making the right decision ?', 'i (24m) react too intensely when my husband (23m) has a problem - how do i calm down ?', 'r/relationships i need your perspective', 'should i (24f) remain friends with my ex boyfriend (32m) ?', 'am i (m23) getting overly attached too quickly ?', 'how do i (24m) stop reacting so intensely ?', 'i (30f) have a weird (abusive ? ) relationship with my boss (36f) and may need to quit abruptly . no idea what to do']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents = relationship_data.split(\" <END> <START> \")\n",
    "relationship_data_sents[0] = relationship_data_sents[0].replace(\"<START>\", \"\")\n",
    "\n",
    "print(relationship_data_sents[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max post length:  73 \n",
      "\n",
      "\n",
      "['sister', '(11f)', 'sleeps', 'beside', 'my', '(26m)', 'used', 't-shirts', 'because', 'it', 'helps', 'her', 'sleep', 'while', 'im', 'not', 'at', 'home', '.', 'i', 'find', 'it', 'uncomfortable', 'but', 'also', 'im', 'not', 'sure', 'what', 'to', 'think', '.', 'is', 'this', 'normal/ok', '?']\n"
     ]
    }
   ],
   "source": [
    "relationship_data_sents_words = [ws_tk.tokenize(post) for post in relationship_data_sents]\n",
    "\n",
    "print(\"Max post length: \", max([len(post) for post in relationship_data_sents_words]), \"\\n\\n\")\n",
    "\n",
    "print(relationship_data_sents_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing each post, within each post is a list of each token within the post. The longest post is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
